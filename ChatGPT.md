
# [ChatGPT](https://www.dedao.cn/course/article?id=wgpMLla6Py4qK25PeQXYmvNzjd2Zx1)

ChatGPT是什么？我先下个判断：它是一个被神化的语言模型的衍生品。

今天，ChatGPT使用的语言模型采用了深度学习算法进行训练，但是这几十年发展下来，语言模型还是语言模型，不是什么其他的东西。

媒体从来是扮演加速师的角色，在赞扬的同时贩卖恐惧。绝大部分赞誉是炒作，这并不可怕，因为20世纪技术的发展常常伴随着泡沫。但几乎所有的恐惧都是自己在吓唬自己，这才是可怕之处。我不知道如果贾里尼克能活过来，会如何评论这种现象。如果他想给大家降降温，大家是否会说他老糊涂了，跟不上发展了。

但是稍微了解一点宗教历史的人都会知道，只有神具有让人们既赞誉，又恐惧的特点。ChatGPT已经成为了一尊新的神。

很多人今天对ChatGPT的反应，让我想起了当年哥伦布的一则故事。

1503年，哥伦布的船队到达了牙买加，岛上的原住民对哥伦布他们表示欢迎，并为他们提供食品。但由于有水手盗窃了当地人的物品，几个月后，当地人不再为他们供应食品了。

哥伦布是一个对天文和地理颇为感兴趣的人，他航海时恰巧带了一本天文年鉴，里面记载了1475到1506年月食出现的时间。于是哥伦布就想到，可以借此扭转不利局势。

在一个将要发生月食的夜晚，哥伦布约见了当地的部落首领，告诉他，神对当地人不招待哥伦布和船员的事情愤怒了，神会使月亮“愤怒地发红”然后消失，以表达惩罚。

果然，当晚月亮变红了，然后逐渐消失。哥伦布的儿子斐迪南在航海日记这样写道：

“（土著人）从四面八方赶到船上，悲伤地哀嚎著，带着食品和各类供给，恳求他们向神尽可能地说情，不要将怒火发泄在他们头上……”

这时，哥伦布走进船舱去假装“祈祷”，其实是在用沙漏计时，当长达48分钟的月全食快结束时，他告诉惊魂未定的原住民，神即将原谅他们。不久后，月球开始走出地球阴影，露出了亮光。哥伦布对说，神已经宽恕了他们。此后原住民就把哥伦布当成了神的代理人，老老实实地给他的船队提供食物。

世界上各种自然神，都是人创造出来的，但是人从此匍匐于它们的脚下。而人这么做，是因为不知道各种自然现象背后的原因，简单地讲，就是不懂得科学原理。

当ChatGPT这尊新的神被树立起来的时候，我不知道有多少人想成为哥伦布，又有多少人的行为和当年的土著人没有差别。如果想成为前者，就需要知道ChatGPT背后的科学原理，就如同需要掌握基本的天文地理知识一样。

很多人一听说了解科学原理就失去了耐心，希望我马上给出一个答案或者行动指南。但根据我对我这十几年来上百万读者的观察，凡是能够耐心听我讲讲道理的人都进步了，凡是懒得动脑筋，想很快得到一些结论或者行动指南的人，还在原地踏步。

这是一门让你成为哥伦布，而不是土著人的课程，它不长，也不难懂，里面没有任何公式，而是一个讲原理的课程。听完之后，你会知道ChatGPT以及类似的产品是怎么一回事，它们能做什么、不能做什么，这就如同哥伦布懂得月食的原理。听完这门课，你会像哥伦布一样，当看到别人在朝拜月亮时至少不会跟着起哄。

今天ChatGPT让人感到神奇的地方主要有两个：

一个是它能回答复杂的问题。另一个是它能写作，你告诉它写什么，它就能给你写一篇能交差的公文。

什么叫难的问题？对计算机来说，所谓的容易问题，就是跟事实有关的问题，比如“是谁”“什么时候”“在哪里”“发生了什么事”等等。容易问题的回答，早在2008年前后就实现了。而所谓“难的问题“，是指”为什么”和“怎么做”这两大类问题。2014年，机器能够完美回答40%的“难的”问题，而现在，ChatGPT能回答的难的问题要多得多。

现在人们使用ChatGPT的目的大致可以归结为三个：

第一个是信息查询，过去Google是给出有答案的网页链接，但今天ChatGPT是直接给出答案。

第二个是让它帮助写作业，这是很多大中学生使用它的原因。

第三个则是写一些应付差事的汇报，这主要是职场上的人士使用。

这三件事的核心是两个，一个是理解自然语言，明白人的意图；另一个是产生自然语言的文本，满足人的要求。

虽然GPT自称是可以自己产生训练数据，但实际上，它要依赖人为产生的初始数据，才能产生新的数据。没有好的初始数据，它产生的数据也是乱七八糟的，当然，训练出的模型质量也就无法保证了。

比如我们前面提到，计算机很早就能帮助人类写财经文章，这是因为这方面的数据多，而且这类文章格式固定。今天一些人觉得，用ChatGPT写每周汇报很方便，也是因为这种汇报的格式很固定。

<img width="1001" alt="image" src="https://github.com/user-attachments/assets/369a01d2-5899-49ed-bb80-751b81899457">

ChatGPT的底层技术是50年前的。

根据香农确立的现代通信原理，所谓的通信，也被称为信道的编码和解码，无非是信息源先产生一个原始信息，然后在接收方还原一个和原始信息最接近的信息。

比如，你传输一句话，“中国是一个古老的国家”。在传输前要对它进行编码，比如编成010101111000…...但是，传输中一定会有噪音和信号损失，接收方接收到的编码可能是1010111000…...这样就翻译不回原来的句子了。

那怎么办呢？我们可以把和接收到的编码相似的句子都列举出来。比如：

国中是一个古老的国家

中国是一个古老的国家

国是一个古老的国家

中国一个古老的国

等等。

然后，通信系统会计算哪一种可能性的概率最大，然后把它选出来。只要传输中的噪音不是太大，而且传输的信息有冗余，我们就都能复原原来的信息。贾里尼克做研究的时候，已经有很多通信的算法来做这件事


从这个角度来看待语音识别也是一样的。当人和人交谈的时候，我说“中国是一个古老的国家”这句话，在空气中或者电话线上传播的是声音的波形，而在接听者那里听到的，其实是带有噪音的声音，他需要接收声音的波形，来还原讲话人说的话。只要噪音不是太大，人是能够做到这件事的。

比如在刚才讲的例子中：

“国中是一个古老的国家”的概率是0.05，“中国是一个古老的国家”的概率是0.2，其它几个候选句子的概率都是0.01，于是我们就认为，概率最高的“中国是一个古老的国家”，就是讲话人讲的句子。

爱动脑筋的人会马上指出，如果我想讲的就是那些小概率的事情呢？你用最大的概率来预测不是就错了吗？

这个观察很敏锐，这确实是语言模型的问题。而解决这个问题的办法就是利用更多的上下文信息，消除所有的不确定性。

比如，第一代语言模型用的上下文信息就很少，但是到了GPT-3，就用到了前后2000个词的信息，包括标点符号等，都算成是词。

由于自然语言中有信息冗余，在这么多上下文里，几乎就不存在不确定性了。这也是为什么今天ChatGPT产生的语句，已经很像人说的话了。但从本质上讲，它的原理依然是在很多种候选中，选择一个概率或者是可能性最大的句子

这个概率该怎么计算，或者说好坏该如何评估呢？

早期的语言模型只是看上下文。还拿“中国是一个古老的国家”举例子，在这句话里，“中国”这个词就比“国中”放到开头要来的通顺。什么叫通顺？就是大家都这么说，用的时候多，并不一定要符合语法。事实上，在生活中很多常见的说法其实都是病句，但是语言模型不考虑这种情况，它只是认为人们说得多的就是好的句子。

那怎么准确计算这个概率呢？就要做一些统计了，统计一下在相同的上下文的条件下，每个词具体出现了多少次。因此，语言模型也被称为是统计语言模型，因为它模型的参数，也就是用来计算各种概率的参数，都是靠统计得出的。

讲到统计之前，我们先要做一个说明，就是今天对于语言模型参数的统计并不是简单的数数，而是要用很复杂的机器学习方法反复计算。我们后面会讲语言模型的三个发展阶段，就是根据如何得到模型参数来区分的。但是为了简单起见，我们可以先把它理解为数数。

为了统计出语言模型的参数，就需要事先准备好大量的文本供统计使用。

比如有两个句子，“天为什么是蓝色的”和“天为什么是绿色的”，哪一个概率更大？

我们很容易想到，“蓝色”的概率更大，因为我们经常会在文本中看到这句话，比如在1亿篇文章和书籍的章节中看到了100次，它的概率就是百万分之一。而后一句话没有看到，概率就认为是零。

但是，如果某句话，比如“天为什么是黄色的”在统计的文本中出现了一次，是否我们可以认为它比那些没有出现过的句子概率大呢？这就不好说了，因为这些小概率事件，出现不出现，都有很大的随意性。出现两次的随机事件，也未必能说明它比出现一次的随机事件发生的概率更大。

为了避免这种所谓小概率事件所带来的噪音，我们能做的就是增加数据量。

2000年前后，我在训练语音识别所使用的语言模型时，只用到了几千万个英语的句子。但是到了2012年，我负责开发计算机问答时，训练的数据就扩大到当时互联网上能找到的全部、上百亿个句子，也就是说，十多年增加了上千倍。

今天，ChatGPT的语言模型所用的训练数据量也是很大的，第一个版本使用的GPT-3用了大约5000亿个词，换算成句子大约是500亿个。GPT-4因为模型规模增加了将近一个数量级，训练数据可能增幅更大了。

当然，提高语言模型的准确性，光增加数据量不够，还需要保证数据的质量

如何利用语言模型写唐诗。

唐诗大多属于韵律诗，它不仅最后一个字押韵，而且每一句诗都是按照一组两个字或者三个字的单元构成的。

比如王之涣的《登鹳雀楼》：“白日依山尽，黄河入海流，欲穷千里目，更上一层楼。”每一句都可以拆成“2-3”组合。杜甫的《登高》：“无边落木萧萧下，不尽长江滚滚来。”每一句都可以拆成“2-2-3”组合。

于是，我们就把所有的绝句和律诗拆成“2-3”或者“2-2-3”组合，然后以两个字或者三个字作为基本的统计单元进行统计，得到它们在上下文中的概率，然后就可以写出一句句概率比较大的诗句了。

当然，我们还要再根据每一个词的语义，把唐诗中出现的词归类，让一首诗中的每一句符合同一个主题。这里面还有一些具体的细节，我就省略了。总之，只要搞懂了语言模型的道理，有现成的诗供学习，让计算机写诗并不是一件很难的事情。

类似地，让计算机写每周汇报，方法也大致相似，因为这类文章几乎都遵循固定的模版。美国很多作家在测试了ChatGPT后，对它进行了逆向工程，认为它是按照五段论写作的，也就是包括开头和结论，以及中间的三个要点，当然这个要点可以增加或者减少，要点之间再做一些承接和转折。五段论是美国初中教学生们写作的基本方法，美国大部分篇幅不长的公文，比如电子邮件，都是这么写的。

<img width="1016" alt="image" src="https://github.com/user-attachments/assets/c8597b67-e983-4773-aa87-3193f99d70c1">

也就是说，一个人使用语言模型的过程中，他个人输入的信息和间接提供的反馈信息，能够被用来改进语言模型。

比如，我们用语言模型构建一个拼音输入法，使用者在不断输入中文时，他输入的中文文本就被用来做训练数据了。这样一来，这个输入法软件就会越用越聪明。

当然，这种做法不仅需要个人数据，还需要大量的存储空间。因为这时候，不同的人使用的语言模型就已经不同了。

今天的ChatGPT其实就有这个功能，它会根据用户的输入不断迭代优化。你如果用它写几篇文章，然后不断对那些文章进行修改，它就会学到你的行文习惯，然后根据你的行文习惯输出内容。但是，如果人们刻意误导它，输入垃圾信息，它输出的文本的质量也会跟着下降。

刚才说了，挖掘语言中更深层的信息，能有效提高语言模型的能力，但与此同时，也让计算量变得巨大。因此，为了让计算机更有效率地工作，在2000年之后，很多语言模型的研究工作就转到提高算法效率上了。

比如，我在博士工作的后期，花了一年多的时间改进训练方法，将计算量减少了两到三个数量级。也就是说，只需要之前千分之一到百分之一的计算资源，就能做同样的事情。

到了2010年前后，Google开发了深度学习的工具Google大脑：一方面，能够更有效地利用计算资源，这使得语言模型能够越做越大；另一方面，也让模型计算出的概率越来越准确。语言模型的发展由此进入第三阶段。

在随后的几年里，Google做了一件事，就是把之前的语言模型重新训练一遍。在训练数据没有增加的情况下，模型的准确性提高了很多，相应产品的质量也提高了。

由于数据量的增加和语言模型准确率的提高，到了本世纪的第二个10年，计算机就逐渐做到了从写一句话到写一个完整的段落。

在此之前，无论是语音识别，还是机器翻译，你都需要给它一个完整的句子，让它生产一个具有同等信息量的句子。比如，你给它一句中文，它产生相应的英文；或者你给它一个句子的语音，它产生相应的文字。输入输出是1:1。

而当语言模型足够强大之后，你给它一点输入，它就可以产生大量输出了。

比如，你问它一个问题，可能只有一句话，它生成的答案可能有一大段文字。你给它几个关键词，让它生成一份简历，它可能会给你产生一页纸的内容。

了解信息论原理的同学肯定会知道，输入的信息量少、输出的信息量多，这中间的差异必须有办法给补回来，否则输出的内容里一定会充满不确定性。

补充的信息从哪里来呢？实际上，都是存在于语言模型中的，需要先把信息输入语言模型。这时的语言模型就是所谓产生式的了。GPT中的G，就代表Generative，产生。

比如，GPT-3的训练数据就包括几十万本图书、几乎全部维基百科的内容。这么多的数据还只是它所用数据的五分之一不到。实际上，单是维基百科的内容，几乎就能回答今天绝大部分人提出的所有有意义的问题了。因此，ChatGPT的表现比很多人好并不奇怪，因为人一辈子学习的内容可能连它的千分之一都没有。

当然，光有信息还不够，为了保证一段文字中各个句子之间的连贯性，还需要语言模型能够计算出几个句子出现在一起的概率。在段落这个层级进行计算，要比在句子这个层级计算，计算量大出成千上万倍，而不是几倍几十倍。

同时，为了让语言模型产生的段落不太发散，通常还要用一些模版对生成的文字进行限制。这也是为什么ChatGPT写汇报和简历能写得很好，写不受限制的文章就显得非常枯燥的原因。

类似的，今天的写作软件通常只能应付有套路的写作要求，比如写财经报道、写大学申请材料等。所谓的“套路”，其实就是计算机使用的模版。今天，美国的作家群体普遍认为，ChatGPT只能按照一定的模版写作。

当然，要让ChatGPT能够回答好问题，或者和人进行对话，还需要让它理解人的问题。这件事相对写作本身要容易一些。

今天，计算机对自然语言做语法分析，可以做得比人还准确。通过句子的语法结构，可以理解常见的实词，也就是名词、动词和形容词的含义。通常来说，句子的含义是这些词的含义的叠加，于是，它就能理解这句话的含义。

当然，在丰富的人类语言中，也经常有例外，就是一句话中所有的词你都懂，但是这句话听不懂。对于这种情况，计算机也没有办法，这时候，它就会胡乱回答，甚至给出完全相反的答案。

<img width="1004" alt="image" src="https://github.com/user-attachments/assets/94fd99c9-a091-490d-948f-1f99cc359234">

ChatGPT真正可怕的地方在于，按照当前的速度发展下去，不断扩大应用领域，它可能可以解决很多原本需要人类才能解决的问题。

语言模型能做的事情分为三类：

第一类：信息形式转换

第一类是将信息从一种形式转换为另一种形式，无论是语音识别还是机器翻译，都属于这一类。

在语音识别中，输入的信息是语音声波，输出的信息是文字，它们是一一对应的，因此是信息在形式上的转化。机器翻译也是如此，是从一种语言的编码，转换成另一种语言的编码。

不过值得指出的是，任何形式的信息转换通常都会损失一些信息。比如，在机器翻译中，语言中所蕴含的文化常常就损失掉了。这倒不是机器的问题，在用人进行的翻译的时候，也经常会出现这种现象。比如，唐诗翻译成英语往往就显得乏味，英文的诗歌翻译成中文，也常常显得平淡无奇。有些贯通中西的翻译家，会试图把文化的元素加回去，但是计算机做不到这一点。

在这一类事情中，一个通常不被人们注意的应用是在医学领域，比如基因测序。

任何物种的DNA都是四种碱基ATCG的组合，当然，它们不是随意排列的，并非所有的组合都是合理的。比如，不同物种同一功能的碱基片段其实是差不多的，每一个基本的单元就有点像文本中的文字。因此，根据一段碱基，有时候就能识别下一段碱基。

当我们进行基因测序时，要把缠绕在一起的DNA序列剪开，一段段地识别。而剪开的时候，就有可能剪坏，因此，通常都是把DNA复制很多份，剪开以后做对比，以免每一份都没有剪好，识别错了。

当然，对于没有剪好，或者识别得不是很清楚的片段，就可以通过语言模型识别、纠正错误。只不过这项工作所使用的语言模型是基于碱基对的，不是基于文字的。

此外，还有一件事也属于这个范畴，就是让计算机写简单的程序。

2014年，著名的机器翻译专家奥科在离开Google之前领导过这样一个项目，就是让人把要做的事情描述清楚，然后让计算机写Python程序。

奥科的想法很简单，既然能够让计算机将一种人类语言的文本翻译成另一种人类语言的文本，就应该能将自然语言描述的文本翻译成机器语言的脚本，也就是程序。

在2014年的时候，奥科的团队已经能把功能描述清楚的简单任务书变成Python程序。不过，当时的困难是，人其实也无法把自己的想法非常准确地用自然语言写清楚。

从信息论的角度看，如果有了完美的算法，这一大类问题都可以得到完美的解决。对于这些事情，最终人是做不过机器的。

第二类：根据要求产生文本

语言模型能做的第二类事情是根据要求产生文本。今天ChatGPT做的主要工作，像回答问题、回复邮件、书写简单的段落，都属于这一类。

这一类工作，输入的信息量明显少于输出的信息量。从信息论的角度看，这会产生很大的不确定性，需要额外补充信息。而补充的信息的来源，其实就是语言模型中所包含的信息。因此，如果语言模型中包含了某个话题的很多相关信息，它就可以产生高质量的文本；否则，它给出的答案或者所写的内容就不着边际。

这一类应用对于语言模型来讲是最难的。这倒不是因为语言模型做得不够好，而是因为站在信息论的角度看，不可能通过少量信息得出更多的信息。因此，这类工作其实或多或少都需要人工干预。

今天，除了ChatGPT，还有很多类似的写作软件，它们写出来的内容看上去都不错。但是，在这些软件背后，其实有一个由人组成的编辑团队，他们会从几十篇候选文章中挑出一篇提供给用户。今天，在硅谷地区还有一些评估内容质量的外包公司，他们有专人评估计算机产生的文本质量，然后反馈给计算机继续学习、改进。

我就ChatGPT的写作水平，专门询问了两位ChatGPT的深度用户。他们本身就是研究机器学习的博士，出于工作的需要，天天都在分析ChatGPT的写作水平。

他们告诉我，一种最大化的发挥ChatGPT写作能力的做法，就是你和它反复迭代。他们是这样做的：

先给ChatGPT提要求，让它写一篇文章。绝大部分人到此为止了，但是他们会对机器写的文章提出新的修改要求，然后它就会重新给你写，然后你再提要求。这样一来二去，几次迭代下来，文章质量就大有提高了。

这两个人一个是美国人，一个是中国人，他们对ChatGPT最终写出来的文章评价差异还是很大的：美国人认为，质量一般，可以作为邮件发出，但不精彩，不能作为自己的写作，否则别人会觉得自己水平太低；而中国人因为母语不是英语，觉得它写得不错，省了自己很多时间，虽然同样水平的文章他也能写出，但是可能要花更多的时间选择用词和语法。

当然，有人可能会觉得，ChatGPT对于一些专业问题给出的答案，甚至比专家还好。这种现象是存在的，正如我们前面所讲，它学过的知识可能是我们的成千上万倍。但那是因为其他专家已经就所提出的问题进行过了研究，有现成的知识可以提供给它。

比如，你如果问计算机“天为什么是蓝色的”，能得到完美的答案，那是因为之前有物理学家进行了研究，并且他们的解释得到了更多物理学家的认可。也就是说，还是有人工干预在先。甚至于很多问题，其实在互联网上就有比较好的问题答案配对。ChatGPT这一类软件只是把它们整理出来。

相反，硅谷几家大公司的研究发现：ChatGPT做小学算术应用题，甚至参加一些语文考试，比它参加高中的AP课考试，以及研究生入学考试，比如医学院的MCAT考试，成绩要差得多。原因就是，那些小学生的题它没见过，而AP课和MCAT考试都是标准化的，有很多过去的考试题可以找到。

不过，虽然ChatGPT不能自己创造答案，但它还是很有价值的，它可以减少人的工作量。这就如同你在参加物理考试时，计算器可以节省时间一样。但是如果你不懂物理学的内容，即便有了趁手的工具，也照样考不出来。

第三类：信息精简

语言模型能做的第三类事情是把更多的信息精简为较少的信息。

比如，为一篇长文撰写摘要，按照要求进行数据分析，分析上市公司的财报，都属于这方面的工作。

这一类工作，输入的信息多，输出的信息少，因此只要算法做得好，就不会出现信息不够用的问题。

将信息由多变少，就会面临一个选择，选择保留哪些信息，删除哪些信息。

比如，为一本书写摘要，不同的人会写出不同的摘要，他们对于书中哪些是重点内容、哪些是次要内容会有不同的看法。类似的，对于某个上市公司的季度财报，不同的分析师会有不同的看法，他们会按照自己的想法挑选数据作为证据。

同样的，把更多的信息进行精简，也会得出不同的结果，这就要看算法是如何设计的，它所依赖的语言模型之前都统计过什么样的信息等等。

对于这一类工作，最终计算机会做得比大部分人更好。这不仅是因为计算机阅读和处理数据快，语言模型强大，更是因为它在做摘要、做分析或者剪辑视频时，能够做到比人客观。

比如，今天很多人分析财报，会有先入为主的看法，然后根据自己的看法选择数据，有意无意忽略那些重要但不合自己想法的数据。还有很多人在做摘要时，喜欢断章取义。这些问题，计算机通常都能够避免。

但是，计算机的算法也有一个问题，就是缺乏个性化。

我们人类，通常不同的人对于同一本书会有不同的看法。同样是阅读《红楼梦》，有的人把它当作宝黛爱情故事来读，有的人把它当作官僚家庭的生活来读，也有人将它当作中国农耕社会的缩影来读。类似地，同样是将一部电影剪辑成短片，不同人挑选的片段也会不同。

但是，机器做这种事情，结果都是千篇一律的。

这就如同生产线出现之前，手工制作的产品，每一件都有自己的特点；而大机器生产之后，所有的产品都是标准化的。

但是总的来讲，在这方面，人是做不过机器的。这就如同绝大部分手工产品的质量都不如大机器生产的好那样。

2019年的时候，著名的人工智能专家（也是我的师兄）郭毅可院士做过这样的估计：

2024年，计算机能够对描述非常清晰的任务进行编程；

2026年，完成中学生水平的作文；

2028年，编辑视频；

2049年，创作最畅销的小说。

郭毅可院士自己还做了一个项目，就是让计算机根据歌曲《东方之珠》生成了一部几分钟的电视片，并且在香港表演了。

这个电视片的内容和画面，实际上是对《东方之珠》歌词的理解和视频化翻译。很多视频画面超出了人的想象，但也有些地方理解得还很肤浅，停留在字面上。比如，它把东方之珠的“珠”字理解为大水滴，因此在唱到“东方之珠”时，画面是一个不断变化的水滴。

了解了ChatGPT能做什么事情，擅长做什么事情，我们就知道如何取长补短了。比如，今天有很多从别人文章中摘出几句话，做一个短视频到抖音上挣流量的人，今后这些内容就不用人来做了，机器可以做得更好。


<img width="999" alt="image" src="https://github.com/user-attachments/assets/4fa2f020-2bb8-4a5b-8836-319376360b77">

对计算机来说，除了“为什么”和“怎么做”的问题属于复杂问题，其他问题都属于简单问题。

比如现在2023年，你问ChatGPT美国总统是谁，它会很快告诉你是拜登。这是有明确答案的。这类问题，计算机根据疑问词和主题词就能理解了，比如这个问题的主题词是“美国总统”，疑问词是“是谁”，再去统计网上关于这个问题的答案，我们就可以建立一个关于美国总统的语义框架，或者说知识框架。

利用语言模型回答问题，不是一个问题对一个答案这样简单的匹配，而是对于问题给出多个答案，然后根据答案的概率排序，返回一个最可能的答案。比如过去的媒体上还有“美国总统特朗普”“美国总统布什”这样的信息，它们也会被统计进去。但是今天的语言模型都很聪明，它们会给最近的内容赋予较高的权重。最后你得到的答案，就会是当前美国总统的名字。当然，如果正好赶在换届时，数据还没有更新，给出的答案就可能出错。

其实都不一定是ChatGPT，今天用苹果手机的Siri功能，或者Google的Google Voice功能，几乎可以回答所有简单的问题，基本上就是按照这种方法去做的。

讲到苹果的Siri，很多人觉得它比ChatGPT笨多了，即便是很多关于事实的问题，它也给不出答案，直接说不知道。其实这是对美国的商业和法律缺乏了解。苹果作为大的上市公司，是不能随便给别人建议的，否则法律上的麻烦很多。此外，作为一个用户每天依赖的产品，可靠性非常重要，不能时对时错。因此，像Siri这类的产品，做不到就不做了。

人遇到这种问题时，有三种途径能够回答。

第一，你知道答案，直接给出。这种做法，计算机也采用。

比如答案就在某个问答网站内。过去计算机采用网页搜索，把那个网页提供出来。今天ChatGPT则是把相应的一段话抽取出来。

为什么今天和过去采用的做法不同呢？原因有两个。一个是过去计算机对文本进行摘要的能力不够，二是过去的搜索其实很少分析句子的语义，不确定用户的问题和问答网站上的问题是否一致。

比如你问“为什么最近苹果的股票不涨”，网页给你一个三个月前的网页，分析“为什么苹果的股票在涨”，因为这里面主要的关键词都能匹配上。你遇到这种情况会哭笑不得，觉得搜索质量很差。今天ChatGPT足够强大，大部分时候不会犯这样明显的错误。

我们回答复杂问题的第二种情况是，你不知道答案，但是你懂得找到答案的基本知识，于是你利用你的基本知识推出了答案。

比如当有人问你天为什么是蓝色的，你学了中学物理，知道太阳光是由七色光构成的，不同颜色的光折射率不同。同时你还能活学活用，想到阳光进入大气层时因为折射率不同会发生散射，导致天空显得是蓝色的。

但是，计算机其实没有这个能力。硅谷的几家著名公司（这里我就不点出名字了），对ChatGPT进行了全面的测试，发现它回答小学常识课（美国叫自然课）的问题，正确率还不到60%。为什么呢？因为这部分问题很少在网络上被讨论，或者网络上没有靠谱的答案，而ChatGPT没有办法像人那样运用知识去寻找答案，只能从现成的答案里归纳总结。

人类解决复杂问题的第三种情况是，你不知道答案，而现有的知识也无法直接推导出答案，需要你做研究工作。

比如这几年疫情，市面上能找到几十种口罩，有些管用，有些不管用，有些虽然管用但是效果有限，到底哪种才靠谱？虽然你能够在互联网上找到一些这方面的内容，其实一大半都是过时的，甚至是想当然的、错误的。过去从来没有人对所有的口罩进行对比研究。这个看似并不复杂的问题，其实并没有好的答案。

2020年，斯坦福大学材料学专家崔屹教授的团队进行了全面的研究，彻底回答了这个问题。比如他们发现，真正能够有效过滤新冠病毒的口罩，是靠口罩纤维上的静电力把病毒吸附在口罩上，让它无法进入口鼻，而不是用纤维网把病毒挡住。他们还发现，纤维越细的口罩，防护效果越好，纤维粗的反而防护效果差，这和人们的直观感觉有很大的差异。后来诺贝尔奖得主朱棣文教授给了一个合理的解释，就是静电场和纤维直径的平方成反比，纤维粗的口罩的吸附能力弱。这项研究还有很多发现，比如口罩一旦沾染了潮气，防护力基本上就丧失了。基于这项研究的综述论文发表之后，得到了广泛引用。

你看，对于这样的问题，人类是需要通过实验和探索发现新知，而且更正过去认识错误的。这显然也是ChatGPT做不到的。

ChatGPT如何工作

那么ChatGPT的答案从哪里来呢？简而言之，回答问题也好、写作短文也好，都基于它对现有事实的抽取和整合，或者说归纳总结。

2012年，我们在Google尝试计算机自动问答时，将当时互联网上全部高质量的英语句子，大约有1000亿个，全部做了语法分析。这是一件非常消耗算力的事情，计算量大约是对这些句子做简单统计的上亿倍。完成这项艰巨的任务之后，可以得到两类知识，一个是每一篇文章内自己所包含的知识，另一个则是全网的知识图谱。

在此之前，Google收购了一家专门构建知识图谱的小公司，那家小公司已经完成了对上百万个知识点的总结，并且构建出它们之间的相互关系。在对上万亿语句进行分析后，这个知识图谱得到了很大的补充。这里必须要提一句，这中间的工作依然需要人工干预，无法全部靠机器完成。

后来，负责Google知识图谱的副总裁到了苹果，是今天苹果Siri的负责人。苹果的Siri其实背后也有一个知识图谱。ChatGPT背后也一样，虽然它没有公布如何利用各种知识，但是它下载了整个维基百科，而维基百科有一个现成的知识图谱。

当人们在问答系统中提一个问题后，问答系统会首先在知识图谱中寻找可能答案。对于我们前面讲到的简单问题，只要问题靠谱，答案通常都能直接找到。

但是对于复杂问题，有些可以通过知识图谱中的几个相关联的知识点回答，但大部分就不行了，于是需要回到提供知识的原始网页中去寻找答案。这个过程其实是一个逆向思维的过程，和人的思维方式不太一样。

比如我现在问一个复杂问题，计算机就会去找到几十上百个可能包含答案的文章，然后从这些文章中提取一些语句，构成可能的答案。当然，为了让这些句子凑在一起看上去像是一个有逻辑的、连贯的回答，就需要使用语言模型了。

换句话说，这些候选的文章里有了原材料，而语言模型是一个厨师，将它做成一道菜。如果语言模型质量不高，提供的答案就是几个事实的堆砌，让人看了觉得毫无连贯性可言。ChatGPT之所以比之前的问答系统做得好，并不是它的原材料更多，而是菜做得更精致。

了解了各种问答系统是如何回答问题的，我们再来看看它们如何实现对话与写作。

对话有点像句子中的填空游戏。比如有这样一句话：从去年（ ）开始，考研成了大学生最关注的话题之一。请问中间该填什么词？

对此，语言模型可以给出概率最高的几个词作为候选。比如上半年，下半年，夏天等等，不太可能提示“箱子”、“北京”、“土豆”这些不相关的词。类似地，如果一段话中拿走了一两个句子，今天语言模型也能填回去，只要语言模型足够大、足够好，填进去的内容读起来就通顺。

如果我们从句子层面扩展到段落层面，你说一句话，它说一句话，就是聊天了。它和一段文字中，抽走了一半的话，再用语言模型复原这段话的原理是一样的。由于聊天时你一句我一句，对话是发散的，并不需要下一句话一定是事先想到的，只要前后顺畅，聊天就能进行下去。如果真遇到它填不出来的的句子，它通常会打哈哈，让你提供更多的输入，直到聊天能够继续下去。

至于长篇文章的写作，ChatGPT所做的实际上是模仿同类的文章，不过会用用户提供的新的信息取代旧的信息。比如我们前面提到的那两位深度使用者，他们就是在不断提供新的信息，引导ChatGPT的写作方向。ChatGPT写的作文其实没有什么营养，内容只是比较巧妙的重复。不过客观地讲，今天大部分中学生写的作文其实质量都不高，也不过是把范文抄来抄去，甚至写得还远不如ChatGPT。毕竟，模仿和抄袭，人是做不过机器的。

就在我修改这门小课的时候，阿里巴巴也推出了一个类似ChatGPT的产品让我试用。于是我发现了一个特点，写一些简单的邮件，ChatGPT的行文是美国式的描述，像是从英文翻译过去的，而阿里巴巴的是中国式的口吻。它们的语言质量我不做评论，但这个现象说明一个问题：你输入给它什么训练数据，它就给你写出什么样的文章。

你可能马上会想到一句很有名的话，叫“垃圾输入、垃圾输出”，这正是ChatGPT、甚至今天机器学习方法固有的问题。

<img width="998" alt="image" src="https://github.com/user-attachments/assets/9b7ea025-5fe3-4eef-b3d2-93280cb3e150">

ChatGPT回答问题、写文章，有的时候很靠谱，有的时候又在胡说八道，这些问题能不能解决呢？

我先说结论：有些问题是可以通过改进解决的，有些则是ChatGPT固有的问题，甚至是今天机器学习方法固有的问题，要解决是很难的。

哪些问题能够改进

我们先来看看哪些问题是通过改进能够解决的，这是我们将来能够期望的。

首先，有些错误是因为信息量不够所造成的，这部分错误将来ChatGPT是可以避免的。人类解决的问题会越来越多，互联网上的优质信息也会越来越多。这一点很容易理解，我们就不展开谈了。

不过，这部分问题解决并不会很快，因为人类创造新知识的速度没有那么快，网络上的文字虽然多，但是每年增加的新知并不多。

ChatGPT还有一个问题，就是它创作出来的作文段落读起来显得乏味、不生动，用美国一些高中语文老师的话说，就是味同嚼蜡。这些问题也可以解决，毕竟语言模型的技术还在进步。

实事求是地讲，今天ChatGPT产生的段落，和十年前我在Google生产的段落相比，通顺程度已经好了很多。打个比方，十年前生成的段落相当于初中生的作文，今天有点像初三或者高一学生的水平了。

这样水平的写作已经有了一定的应用价值。比如中国学生申请美国的大学，英语作文一定是写不过美国高中生的，只要你给它规定清楚写什么，用ChatGPT写可能比自己写得还通顺些。考虑到自然语言处理技术进步比较快，ChatGPT的写作可能很快会超过高中生的平均水平。

另一个有希望解决的问题是，今天的ChatGPT还离不开后面大量的人工，而且运行的成本特别高。这个问题，随着算法的改进是可以慢慢解决的。举个例子，战胜李世石的那一版AlphaGo，消耗的能量是一栋十几层办公大楼的耗电量，但是两年后战胜柯洁的那个版本，耗电量就减少了两三个数量级。

已故的著名物理学家张首晟教授讲的一句话：很多人做了多年研究，结果把物理学的第一性原理都忘了。

如果你不给它提供足够多的信息，它就无法做事情。这就如同你不给汽车提供燃料，它就无法走一样。

从原理上讲，今天几乎所有的人工智能产品都是复读机。先要有各种知识和信息，ChatGPT才能工作。你给它提供高质量的数据，它就会产生一个高质量的语言模型，然后给出高质量的答案，写出高质量的文章。相反，你用垃圾数据训练它，它就输出垃圾。

什么叫垃圾数据呢？其中一部分就是噪音。上个世纪90年代，美国语音识别的科学家就发现，如果在安静环境下，用麦克风录制语音进行识别，错误率很容易降到10%以下。但是，如果是电话录音，由于电话有噪音，错误率会超过30%。对于语音来讲，噪音就是一种垃圾。

在2000年前后，我在约翰霍普金斯大学和Google都做过这样一些实验，把训练语言模型的文本混入一些噪音，比如我们在文本里混入一些错别字，或者把一些字的次序交换一下，语言模型的质量会大大下降。

另外，噪音越多，质量下降的速度越快。比如我们用语言模型进行从拼音到汉字的转化，当噪音在1%、2%时，不太会影响转换的准确率；噪音到了5%时，错误率就会明显上升两三倍；当噪音达到10%的时候，错误率就会上升十多倍。如果噪音更多，语言模型就不起作用，产生的结果就是随机的了。

使用过ChatGPT的朋友会有这样一个体会，当你和它谈论一些有争议的话题时，它给出的回答可能非常不靠谱，前后自相矛盾，完全没有逻辑，甚至和话题不沾边。为什么会这样？因为在网上关于那个话题的讨论，本身就非常不靠谱，而ChatGPT学习了那些内容后，会将不靠谱的表现放大。这就是机器学习中很有名的那句话：垃圾输入，垃圾输出。

有没有什么解法呢？通常只有两个做法。

第一个做法是在噪音不太高时，增加训练的数据量，这个做法是有效的，但是需要多用好几倍的训练数据。比如语料库里混有了5%的噪音，将语料库的规模增加十倍，可能可以弥补这个不足。

第二个做法是在能够找到噪音来源时，过滤掉噪音。比如你在开车时使用苹果的Siri服务，其实引擎的声音会进入到麦克风，但那是固定频率的噪音，Siri会将它过滤掉。再比如，我们发现某个网站的内容一直不靠谱，就将相应的内容删除。

但是，相当一部分噪音是随机产生的，我们今天还是无能为力。这是今天机器学习的一大问题。

除了噪音， 今天机器学习还有一个问题是：它所依赖的正反馈，有时会将它引向歧途。

什么是正反馈呢？比如你在短视频网站上看了几个NBA篮球的短视频，系统就得到一些反馈，觉得你会喜欢类似的视频，然后就调整了特别针对你的推荐模型，多给你推荐NBA的节目。这就是系统自适应的正反馈。这种做法通常让使用者觉得非常贴心，越用越好用。很多人对短视频和无厘头的推文上瘾，就是这个原因。

但是，自适应的正反馈是把双刃剑，如果有人刻意引导ChatGPT犯错误，这种正反馈机制会导致它错误百出。事实上，很多使用者已经发现，ChatGPT在回答很多问题时已经被人“教坏了”。比如，有的用户可能是处于开玩笑的目的，给它提供了很多做违法事情的信息，以及很多仇恨的信息，结果它会教人如何杀人，如何做炸弹，并且时不时说出种族歧视的言论。

和过去不同的是，今天我们处在一个信息过载的时代。今天人们发愁的不是无法获得信息，而是信息太多，自己看不过来。很多信息还彼此矛盾，让人们无所适从，更不用说那些毫无营养、让人上瘾的视频和推文了。

面对这样一个信息过载、信息和噪音难以分辨的世界，我们自己判断一条消息真假的成本都很高，ChatGPT本身更是无能为力。我在前面讲了，它像是一个厨师，你给它有营养的食材，它有可能做出一道既有营养、味道也还不错的菜。但你给它垃圾数据进行训练，它输出的也只能是垃圾。这是今天机器学习普遍的问题。

这个本身有一万亿参数的模型很难进行人工调整。这个巨大的语言模型就像是一个黑盒子，你无法搞清楚里面那些模型参数的含义。


早期的语言模型比较简单，通常是直接把上下文的概率存在里面。今天的语言模型，存储的是人工神经网络的参数，从那些参数，你完全看不出它们和概率的大小直接的关系。换句话说，你很难通过人为调高或者调低一些参数来控制ChatGPT的输出结果。

人工干预还有一个很大的隐患，就是把人主观的好恶加进了一个原本应该客观的语言模型中，这可能导致更大的不公平。

在此之前，推特的人工干预就造成了很坏的影响。在2020年美国总统大选期间，推特根据自己的好恶，封掉了它不喜欢的特朗普的账号。这显然是在滥用权力。随后，马斯克认定推特的做法违反了言论自由的原则，收购了推特并且赶走了全部的管理层。在完全控制了推特之后，马斯克来了一个180度的大转弯，一方面恢复了特朗普的账号，另一方面封掉很多媒体的账号。

今天的ChatGPT已经是一家平台公司了，如果里面的人随意根据自己的好恶选择训练数据，控制结果。这个危害可能比操控推特更大。

对于一些有明确答案的问题，它还是能回答得很好的。但是对于缺乏有用信息的问题，它给出的答案相当随意，基本上只能是重复车轱辘话，没有太多的价值。根本原因就是这一讲解释的：垃圾输入，垃圾输出。

<img width="1001" alt="image" src="https://github.com/user-attachments/assets/0b96bf50-6f08-454b-9170-c1b01f88a7fb">

ChatGPT的核心是语言模型，而语言模型需要用大量的数据来训练，有了数据后，还需要强大的算力支持，然后还需要有足够高水平的并行计算和机器学习的算法支持。数据、算力、算法三道坎，只有极少的互联网大厂能够越过去。

训练语言模型的三个限制，分别是数据、算力和算法。这三道坎，只有极少的互联网大厂才能越过去。

其实，即便跨过了三道坎，做出了比ChatGPT更好的产品，它也不可能像很多人想象的那样无所不能。因为ChatGPT，乃至人工智能，本身就是有能力边界的，并不能解决所有问题。

<img width="1014" alt="image" src="https://github.com/user-attachments/assets/db769855-e1c5-4513-aef6-f9cfe4821506">

ChatGPT的边界是人工智能的边界，而人工智能的边界是数学的边界，数学是有边界的。在历史上，已经有三个人从理论上划定了数学和计算机的边界。

哥德尔

第一个指出数学边界的人是哥德尔。

在19世纪末，由于科学技术的突飞猛进，人们普遍对科技的能力感到乐观，觉得它无所不能。在数学界，大家普遍觉得，数学可以是一个既严密又完备的知识体系，甚至认为所有的问题都能用数学的方法解决。

当时，大数学家希尔伯特，就致力于构建一个既完备又一致的数学体系。通俗地讲，所谓“完备”，就是数学的公理可以推导出所有合理的结论；所谓“一致”，就是推导出的结论不矛盾。当然，在数学上，完备性和一致性都有严格的定义，我们就不细究了。

但是在1931年，年仅25岁的哥德尔证明了，数学体系不可能既完备又一致。也就是说，保证了完备性，结论就会矛盾；保证了一致性，就会有很多结论无法用逻辑推理的方法证明。

这对当时的数学界，特别是年迈的希尔伯特本人，是一个异常巨大的打击。但是，这也唤醒了人们，让人们知道，数学不是万能的，世界上很多问题不是数学问题。

这个认识很重要，让人类少走了很多弯路。

马季亚谢维奇

第二个进一步限制数学或者说算法边界的人，是俄罗斯的数学天才马季亚谢维奇。

早在1900年的时候，希尔伯特就提出过一个疑问，也就是著名的希尔伯特第十问题，简单说就是：对于某一类数学问题（不定方程整数解的问题），有没有一个方法，通过有限步，就能判断它有没有解？

注意，希尔伯特关心的只是判断一下它有没有解，还没有考虑如何解决它。当然，判断不清楚是否有解，就不可能解决它。

这个问题困扰了很多数学家一辈子。最终在上个世纪60年代末，做大学毕业论文的马季亚谢维奇解决了这个问题，给出了否定的答案。也就是说，即便是对于一些看似很简单的数学问题，我们都可能不仅不知道如何解决它，而且都无法判定它是否有答案。

当时，马季亚谢维奇只有22岁。

马季亚谢维奇的结论对搞计算机的人来讲，既是一个坏消息，也是一个好消息：坏消息是，很多问题就不要想着如何编程去解决了。因为刚才说了，我们压根无法判定它是否有答案，更不要说解决了。换句话说，很多数学问题压根无法用计算机一步步解决；好消息则是，避免了很多人在这里浪费时间。

不过，并非所有搞技术的人都知道数学的边界在哪里，依然有无数人在犯错误，试图解决不应该用计算机解决的问题。

图灵

第三个是给出计算机能力边界的人——图灵。

图灵当时知道希尔伯特第十问题，他觉得答案否定的，但是又无法证明。

当时，图灵还受到冯∙诺依曼的启发。他在读了冯∙诺依曼的《量子力学的数学原理》一书后认为，人的意识来自于测不准原理，但是计算则来自于机械的运动。图灵认为，这就确定了什么可以计算、什么不可以计算。

后来，图灵发明了一种叫作图灵机的装置，能够在有限时间内，判断出哪一类问题能够在有限的步骤内计算出来。

简单地说，图灵不仅给出了可计算的问题，也就是有答案的问题的边界，而且告诉了人们实现计算的方法。这个方法就是图灵机，它能够在有限时间内，判断哪一类问题能够在有限的步骤内计算出来。

人工智能的边界

好，讲完了数学的边界和计算机的边界，我们再来看看人工智能的边界。

首先，世界上很多问题都不是数学问题。

比如，你今天在一条单行道上遇到一辆无人驾驶的汽车，你故意挡住道不让它过去。这时候，它就没有办法了，它使尽所有算法都无法把你赶走。当然，如果是人开车遇到这种情况，可以下车把前面这个人拖走（当然这不合适），也可以打电话给警察，让警察把他请走。但是这两种做法，都不是数学的解决办法。

其次，根据希尔伯特和马季亚谢维奇给出的结论，很多数学问题有没有解是不知道的。

在计算机和数学领域，知道一个问题有没有解，叫作“可判定”，这样的问题叫作“可判定问题”。

再次，可判定的问题很多是无解的，只有少部分有答案。

![image](https://github.com/user-attachments/assets/e45f5c91-19e3-4079-915d-e548673d908d)

我在文稿里附了一张图，把所有的问题、数学问题、可判定的问题和有答案的问题之间的嵌套关系表示了出来，你可以点开文稿看看。

然后，在有答案的问题中，有一些是可以通过图灵机解决的问题，也就是所谓的可计算的问题。

当然，图灵机是一种理想状态的计算机，它所谓的有限时间，可以是非常长的时间，比如1万亿年，超过宇宙的年龄。

再有，现实生活中计算机能解决的问题，又是可计算问题中一个更小的子集，我们称之为“工程上可计算的问题”。

比如，我们前面说了，如果训练一个模型的时间是20年，我们今天基本上就认为不可计算了，至少是工程上不可计算。

![image](https://github.com/user-attachments/assets/f4602f55-2fc0-4ebd-8a2b-a42cb92d5e5c)


最后，工程上可计算的问题并非都属于人工智能研究范畴的问题，人工智能可以解决的问题，又只是工程上可计算的问题的更小的子集而已。

文稿区还有一张图，示意出了有答案的问题内各种问题集合之间的嵌套关系。可以看出，人工智能可以解决的问题只是世界上所有问题中极小的问题。

而且，即便是这极少的人工智能可以解决的问题，也还必须找到算法才能计算。

今天，人工智能发展得比较快，解决了不少问题，就是因为过去没找到算法的问题，今天算法找到了。但是，这并不意味着现有的算法已经解决了所有人工智能的问题，更不要说解决全世界的所有问题了。

讲到计算机能力的边界，还有一些人会有一个疑问，就是：量子计算出来之后，是否原来计算机解决不了的问题，今天就能解决了？

这其实是混淆了计算能力和可计算性。计算能力增加，原来可以计算的问题会算得更快，瞬间解决，但是不可算的还是不可算。

我们打个比方，你如果有一台压缩制冷机，可以将温度降低。如果你有一个超大功率的制冷机，温度降低得会快得多。但是，用再多、再大的制冷机也不可能将温度降到绝对零度以下，因为那是物理学的一条边界。

在人工智能的边界内思考

那么，为什么今天很多人会在人工智能的边界外胡思乱想呢？有这种想法的人可以分为两类：

一类是真的无知。

今天有一个怪现象，就是完全没有做过人工智能的研究，甚至不知道人工智能为何物的人，就敢自称专家，对人工智能发表各种看法。这些人就如同当年见证了哥伦布让月亮消失又让月亮出现的牙买加原住民，他们会向其它人宣传，“出现了一个叫作哥伦布的神”。

另一类则是人工智能领域的从业者，他们把自己做的贡献在无意中夸大了。

你可以在脑海中想象两个互相嵌套的圈，大圆是今天能够用计算机解决的问题，小圆是人工智能已经解决的问题。我们可以想象，经过一段时间之后，人工智能能够解决的问题变多了，也就是说，小圆变大了。

![image](https://github.com/user-attachments/assets/ccac9b6d-e865-43b9-a0d8-4fcfa94eed64)

但是很多从业者，特别是做出了具体贡献的人，会从自己的角度放大自己的成就。

这些人都只看到问题的局部。事实上，它们看到的工程上能解决的问题和人工智能能解决的问题，根本和现实中不一样，连大小形状都变了。当人工智能发展了一点，小圈变大了，他们就觉得，人工智能似乎已经解决了绝大部分可解决的问题，于是他们或兴奋，或慌张。

不客气地讲，今天很多研究具体问题的人不抬头看路，只看到自己眼前的一点点研究领域，把一点点进步当作很大的进步。这种看法，和庄子所描绘的井底之蛙没有太大的差别。

![image](https://github.com/user-attachments/assets/30ecb6fe-52a4-42bc-8820-730da8bb5397)

事实上，很多人是没有耐心了解原理的，也因为不了解原理，于是只能人云亦云的跟风。

<img width="1002" alt="image" src="https://github.com/user-attachments/assets/025af154-1e0f-433e-934a-7d574821e5ba">

2023年上半年，ChatGPT有多热，不需要我重复了。你在网上会看到这样的文章，比如：

《ChatGPT揭开AI战幔：杀死黄页一样摧毁Google？》

《ChatGPT强势来袭，打工人“瑟瑟发抖” 又能做什么？》

有人很焦虑，有人庆贺新技术的到来，你会听到各种声音，这很正常。一个新技术出现，各种消息铺天盖地并不是什么新鲜事，抛开新技术本身，这当中肯定有炒作的成分。

如果翻翻10年前的旧闻，你就会发现差不多的文章早就出现过，只不过现在是用ChatGPT替换了当时的一些词语。这几年，有差不多待遇的技术就有VR、量子计算、元宇宙、区块链、Web3，等等。除了VR和量子计算在慢慢取得进展，区块链在一些行业开始得到一点应用，剩下的都还没有什么结果。ChatGPT不是第一个被热炒的话题，也不会是最后一个。

热炒的背后一定有原因，有利益、有恐惧、有哗众取宠，也有无知。我们不妨从投资、从业者、媒体和普通人的角度做一个简单的分析

有人问我，为什么苹果公司对此毫无动静？我回答说，这就是苹果公司能够成为今天全球市值最大、利润最高公司的原因。有人觉得我这种回答显得暮气沉沉，但是我们看数据，苹果公司市值一度超过3万亿美元（2023年还有2.6万亿），在过去的十年里长期占据世界公司的市值、利润榜首，这不是浪得虚名的。苹果能做到这一点，恰恰是因为它谨慎行事，在一切变化面前淡定，不炒什么概念。

当初苹果的iPhone刚出来时，它只有2G的服务，而其他电话已经开始支持3G了。当很多手机开始宣布支持5G时，苹果并没有跟风，过了两年才推出它的5G手机。但最终，苹果赢得了手机的市场。今天有人嘲笑苹果的Siri落后了，很多问题回答不了，但是我觉得，不给答案要远比给出一个错误的答案好。如果ChatGPT这样的服务将来真的很流行，你会看到一个苹果版的，但不是在今天。

如果ChatGPT这件事是一个方向，早一天、晚一天开始这方面的工作差别不大，毕竟追赶别人比自己在前面摸黑探索要快得多。在消费电子展（CES）上，从一个概念原型到挣钱的产品，中间要间隔多久呢？通常要七年时间。而七年前展示原型的，和七年后展示挣钱产品的，常常是两家公司。如果一项技术只有一年的窗口期，不是说明它发展得太快，一年就把全部的问题解决了，而是说明它一年后不存在了。

在这里，我有一个提醒，就是不要天天只看媒体报道去投资，因为这样会看不到世界上真正挣钱的地方。

1929年经济大萧条之前，当纽约街头擦皮鞋的小童向约瑟夫·肯尼迪，也就是肯尼迪总统的父亲兜售股票时，老肯尼迪就知道出问题了。今天，当从来没有从事过计算机行业工作的人开始大谈AI，觉得自己比专业人士更懂，甚至开始对计算机领域的人指手画脚时，我们就要警惕了。

ChatGPT既不像很多人讲的那么神奇，也没有那么可怕，但也不是毫无用途。

<img width="1013" alt="image" src="https://github.com/user-attachments/assets/465f73d5-7898-4092-b514-b7066c57b8a5">

什么人会被取代

会被ChatGPT取代的人有三个特点：从事不费体力的工作，不动脑子的工作，或者不产出信息的工作。

按照收入从高到低排个次序：

1.金融分析师

2.大部分财经媒体记者

3.律师助理

4.普通文员

5.短视频制作人

6.大部分公众号写手

……

这些工作都有一个共同的特点，就是本身不产生信息。

金融分析师所做的工作，是把一些公开的数据根据公式整理一下。只要你稍微留意一下就会发现，几乎所有金融机构的研究报告，都是一半对一半错，没有例外。今天大部分财经媒体的记者，创造的信息也很少。

至于，律师助理过去是为律师们提供信息、整理信息、起草文件，这些事情都是把信息从一个文件中搬运到另一个文件中。至于要什么信息、对什么信息作出什么样的判断，是律师的事情，而不是助理的事情。普通文员做的事情也差不多。

还有今天很流行的短视频，没有营养的占大多数，剩下的还有一多半属于剪辑其他视频。很多公众号的文章也是如此，抄袭甚至断章取义别人的内容，再加上一个耸人听闻的标题。如果是这种内容，ChatGPT产生得更快，甚至比人产生的质量更高。等到ChatGPT的创作普及后，这种无脑赚钱的人好日子可能就不长了。当然，对社会来讲，这或许并不是坏事。这个世界并不需要很多在互联网上哗众取宠的人。

另外还有一类工作，在短期内可能收益还不错，但是前景会越来越暗淡，就是低端的IT从业者。

今天，从事IT开发工作的人，大致可以分为两类。一类是研发新产品和做解决方案的。这一类工作通常比较辛苦，需要接受比较长时间的专业教育，这部分工作不会被取代。危险的是另一类，就是处理日常事务，包括运营、测试、数据处理等等，这些工作只需要经过短期的培训就能胜任。

越是到了各种智能工具不断涌现，做事情越来越便利的时候，从事创造性的工作也就越来越重要。

美国教授给学生写的推荐信就非常个性化，这个学生到底好还是不好会讲得清清楚楚。即便他提到了某些学生的弱点，你也能知道那些弱点是致命的还是无伤大雅的。

我们总是在说要做具有创造性的工作，但其实大部分人是停留在口头上。世界上有很多未知的问题，需要亲力亲为去解决，它们未必都是大问题，但却很重要。比如我们前面提到的，如何知道哪种口罩防护力最高的问题，需要做研究才能搞清楚，不是坐在办公室里凭空推理能解决的。

在历史上，科技越发达，人们受教育越多，会变得越来越聪明，也就是智商测试结果逐年增加。这也被称为弗林效应。但是，这个趋势在上个世纪70年代信息革命开始之后停滞了。随着机器智能越来越先进，智商测试分数反而在逐年降低。美国西北大学和俄勒冈大学科学家从2006年到2018年做了很多这方面的研究，发现不仅美国人的智商逐年下降，而且年轻的一代，也就是18~22岁的人智力已经达不到他们父辈的水平。同时两极分化的现象也非常严重，教育程度较低的人，智商下降幅度更大。不只美国，欧洲多个国家也发现了同样趋势。

也就是说，但凡觉得有了智能技术，自己就不需要努力的人，会首先成为智能时代的牺牲者。人工智能可以解决很多问题，但救不了思想上懒惰的人。相反，有了智能工具，反而更加勤奋的人，不仅不会被ChatGPT抢走工作，还会因此有更高的成就

<img width="1010" alt="image" src="https://github.com/user-attachments/assets/76e5395f-e741-49bd-af69-ca644c410015">

如果它真的成为了趋势，等到趋势明显一点再投身进去也来得及。但凡能成为趋势的技术，都会有几十年的发展机会。相反，如果一项技术，或者一个新领域，只存在几年的时间，错过前两年就没有机会了，那这样的所谓机会不要也罢。毕竟大家用钱投资也好，都是希望有长期回报，而不是仅仅一两年的回报。

这十几年，我印象很深的有这样两件事：

第一件事是十多年前，中国团购“百团大战”时，我们买了一些广告公司的股票，结果挣了很多钱。原因很简单，那么多公司挤到一个并不算大的市场上，大概率挣不到钱，但是它们为了争夺市场，肯定会拼命做广告。最后，成百上千的团购公司烧完投资者的钱就关门了。那么，钱去了哪里？它们成为了广告公司的收入。

第二件事是在比特币热的时候，一些朋友听了我的分析，买了AMD和英伟达的股票，也挣了很多钱。道理也是一样的，挖矿挣不挣钱不知道，但矿机、GPU是一定要花钱买的。

今天，如果你是做高端处理器或者智能化云计算的，也要恭喜你，你挣钱的机会可能比直接做人工智能的更多。

事实上，之前把握住某些机会的人，就不会问这个问题，因为他们是先学会了如何把握机会，然后才把握住机会的。

具有把握机会的能力，任何时候都有机会。没有这个能力，试图搭顺风车，这种好事恐怕轮不到大多数人。因为参与的人太多，就不可能有利润了。

![image](https://github.com/user-attachments/assets/e2f63591-bcff-4bcb-8ed1-161bcc4479d0)

其实，比恐惧的结果更可怕的是恐惧本身。很多人因为不必要的恐惧把生活搅得一团糟。今年年初，一个在上海的年轻人问我：最近行业的情况不好，公司要裁员，我在想，是否该离开上海回老家？我对他讲，不是还没裁到你吗，真把你裁了，再考虑是否留在上海不迟。今天很多人对ChatGPT的恐惧，就是这样一种状态。

对于绝大部分人来讲，有没有ChatGPT，他今后十年的生活轨迹不会有什么变化。因为在任何社会里，大环境都是最重要的。

今天在IT领域获得巨大成功的人，都是因为以PC机、互联网和移动通信代表的IT革命持续了50年，他们获得了50年叠加进步的红利。如果只持续10年的增长，马云、马化腾就都不是今天的境况了。

今天中国人的财富比祖辈、父辈增加了上百倍，是因为有四十多年和平发展的大环境，而不是大家的本事比父辈、祖辈大，或者把握机会的本事高。

专业做股票交易的人都知道，即使交易非常频繁，而且成功率极高的交易员，80%以上的回报也来自于大盘，他自己做的那些努力只是微调而已。

因此，大家将来的命运取决于大环境，而不是某一项现在还看不清的技术。如果大环境不好，技术再进步，红利也落不到大家身上。大家看看今天的拉美、俄罗斯和南非就知道了。相反，如果大环境好，任何技术进步都会变成新的就业机会。

面对一个自己看不清楚的事情，不妨多观察一段时间，再做结论。对于自己不懂的事情，要搞清楚，不要和别人一同去造神。今天人们造了太多的神。对很多人来讲，过去乔布斯是神，今天马斯克是神，将来ChatGPT是神。但其实，造神是一种思想懒惰的表现，因为不用思考，一切归结为神就可以了。

因此，越是别人狂热的时候，我们越需要冷静。未来不是给机会主义者的，而是给踏踏实实把本职工作做好的人的。


<img width="984" alt="image" src="https://github.com/user-attachments/assets/14ae8ceb-fff4-40b3-b7de-ec2db2d3501d">


<img width="1010" alt="image" src="https://github.com/user-attachments/assets/1959e6ad-3b42-406e-94fd-165fcb5caa69">


